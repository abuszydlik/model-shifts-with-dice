{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", message=r\"Passing\", category=FutureWarning)\n",
    "warnings.filterwarnings(\"ignore\", message=r\"Implicit\", category=UserWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "from carla import log\n",
    "from carla import MLModelCatalog\n",
    "from carla.data.catalog import CsvCatalog, OnlineCatalog, DataCatalog\n",
    "from carla.models.negative_instances import predict_negative_instances\n",
    "from carla.recourse_methods import Dice, Wachter\n",
    "from copy import deepcopy\n",
    "from datetime import datetime\n",
    "from func_timeout import func_timeout, FunctionTimedOut\n",
    "from ipynb.fs.full.dynamic_recourse import DynamicCsvCatalog, DynamicMLModelCatalog\n",
    "from kneed import KneeLocator\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\"\n",
    "\n",
    "#     torch.manual_seed(0)\n",
    "#     random.seed(0)\n",
    "#     np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def k_means(data, min_clusters=1, max_clusters=10):\n",
    "    \"\"\"\n",
    "    Applies the k-means algorithm and automatically estimates the elbow point.\n",
    "    The algorithm used to calculate the elbow point is described in 10.1109/ICDCSW.2011.20\n",
    "    \n",
    "    Args:\n",
    "        data (pandas.DataFrame): \n",
    "            Records along with their labels.\n",
    "        min_clusters (int): \n",
    "            Minimal number of clusters that is expected in the dataset.\n",
    "        max_clusters (int):\n",
    "            Maximal number of clusters that is expected in the dataset.\n",
    "        \n",
    "    Returns:\n",
    "        int: Estimated number of clusters that yields the elbow point on an inertia graph.\n",
    "    \"\"\"\n",
    "    clusters = []\n",
    "    scores = []\n",
    "    # Fit different potential numbers of clusters\n",
    "    for k in range(min_clusters, max_clusters + 1):\n",
    "        kmeans = KMeans(n_clusters=k, random_state=42).fit(data)\n",
    "        clusters.append(k)\n",
    "        scores.append(kmeans.inertia_)   \n",
    "    \n",
    "    # Automatically find the elbow point, this should change at some point during the application of AR\n",
    "    # if the counterfactual instances form their own cluster(s), the value returned by this method should change.\n",
    "    kneedle = KneeLocator(clusters, scores, curve=\"convex\", direction=\"decreasing\")\n",
    "\n",
    "    return int(kneedle.elbow)\n",
    "\n",
    "def class_statistics(data, target, aggregate):\n",
    "    \"\"\"\n",
    "    Applies an aggregation function for the two classes described in the dataset.\n",
    "    \n",
    "    Args:\n",
    "        data (pandas.DataFrame):\n",
    "            Records along with their labels.\n",
    "        target (str): \n",
    "            Name of the target column.\n",
    "        aggregate (Callable): \n",
    "            An aggregation function which can be applied on data.\n",
    "        \n",
    "    Returns:\n",
    "        dict: Values return by the aggregation function applied on the positive and negative class.\n",
    "    \"\"\"\n",
    "    features = data.loc[:, data.columns != target]\n",
    "    positive_samples = features.loc[data[target] == positive_class]\n",
    "    negative_samples = features.loc[data[target] == negative_class]\n",
    "    return {\"positive\": aggregate(positive_samples).to_dict(), \n",
    "            \"negative\": aggregate(negative_samples).to_dict()}\n",
    "\n",
    "def measure_distribution(dataset):\n",
    "    \"\"\"\n",
    "    Computes a set of statistical measures for the distribution of a dataset.\n",
    "    \n",
    "    Args:\n",
    "        dataset (DataCatalog):\n",
    "            Catalog containing a dataframe, set of train and test records, and the target.\n",
    "        \n",
    "    Returns:\n",
    "        dict: Statistics calculated for a specific distribution of data.\n",
    "    \"\"\"\n",
    "    num_clusters = k_means(dataset.df.loc[:, dataset.df.columns != dataset.target].to_numpy())\n",
    "    means = class_statistics(dataset.df, dataset.target, np.mean)\n",
    "    stds = class_statistics(dataset.df, dataset.target, np.std)\n",
    "    \n",
    "    return {\"num_clusters\": num_clusters, \"means\": means, \"stds\": stds}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(dataset, hyper_params, retrain=False):\n",
    "    \"\"\"\n",
    "    Instantiates and trains a black-box model within a CARLA wrapper that will be used to generate explanations.\n",
    "    \n",
    "    Args:\n",
    "        dataset (DataCatalog): \n",
    "            Catalog containing a dataframe, set of train and test records, and the target.\n",
    "        hyper_params (dict): \n",
    "            Dictionary storing all custom hyper-parameter values for the model.\n",
    "        \n",
    "    Returns:\n",
    "        MLModelCatalog: \n",
    "            Classifier with additional utilities required by CARLA.\n",
    "    \"\"\"\n",
    "    \n",
    "    model = DynamicMLModelCatalog(dataset, model_type='ann',\n",
    "                                     load_online=False, backend=\"pytorch\")\n",
    "\n",
    "    # force_train is enabled to ensure that the model is not reused from the cache\n",
    "    if not retrain:\n",
    "        model.train(learning_rate=hyper_params['learning_rate'],\n",
    "                    epochs=hyper_params['epochs'],\n",
    "                    batch_size=hyper_params['batch_size'],\n",
    "                    hidden_size=hyper_params['hidden_size'],\n",
    "                    force_train=True)\n",
    "    else:\n",
    "        model.retrain(learning_rate=hyper_params['learning_rate'],\n",
    "                      epochs=hyper_params['epochs'],\n",
    "                      batch_size=hyper_params['batch_size'])\n",
    "        \n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def performance_metrics(dataset, model):\n",
    "    \"\"\"\n",
    "    Computes a set of performance metrics for a classifier.\n",
    "    \n",
    "    Args:\n",
    "        dataset (DataCatalog): \n",
    "            Catalog containing a dataframe, set of train and test records, and the target.\n",
    "        model (MLModelCatalog) \n",
    "            Classifier with additional utilities required by CARLA.\n",
    "    \n",
    "    Returns:\n",
    "        tuple of (float, float): Accuracy and F1 score of the model.\n",
    "    \"\"\"\n",
    "    predictions = model.predict_proba(dataset.df_test.loc[:, dataset.df_test.columns != dataset.target])\n",
    "    predicted_labels = np.argmax(predictions, axis=1) \n",
    "    ground_truth = dataset.df_test.loc[:, dataset.df_test.columns == dataset.target].values.astype(int).flatten() \n",
    "    \n",
    "    return accuracy_score(ground_truth, predicted_labels), f1_score(ground_truth, predicted_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recourse_worker(generator, factual):\n",
    "    \"\"\"\n",
    "    Apply algorithmic recourse for a (set of) factuals using a chosen generator.\n",
    "    \n",
    "    Args:\n",
    "        generator (RecourseMethod): \n",
    "            Generator that finds counterfactual explanations using a black-box model.\n",
    "        factual (pandas.DataFrame): \n",
    "            One or more records from a dataset used to train the black-box model.\n",
    "        \n",
    "    Returns:\n",
    "        pandas.DataFrame: A counterfactual explanation for the provided factual.\n",
    "    \"\"\"\n",
    "    counterfactuals = generator.get_counterfactuals(deepcopy(factual)).dropna()\n",
    "    if not counterfactuals.empty:\n",
    "        return counterfactuals.sample().astype(float)\n",
    "    \n",
    "def recourse_controller(function, max_wait_time, generator, factual):\n",
    "    \"\"\"\n",
    "    Wrapper function that ensures the application of recourse does not run indefinitely.\n",
    "    \n",
    "    Args:\n",
    "        function (Callable): \n",
    "            Function that will have its execution placed under a timeout.\n",
    "        max_wait_time (int): \n",
    "            Number of seconds after which the `function` will time out.\n",
    "        generator (RecourseMethod): \n",
    "            Generator that finds counterfactual explanations using a black-box model.\n",
    "        factual (pandas.DataFrame): \n",
    "            One or more records from a dataset used to train the black-box model.\n",
    "        \n",
    "    Returns:\n",
    "        pandas.DataFrame: A counterfactual explanation for the provided factual if found.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        return func_timeout(max_wait_time, function, args=[generator, factual]) \n",
    "    except FunctionTimedOut:\n",
    "        log.info(\"Timeout - No Counterfactual Explanation Found\")\n",
    "\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_distribution(data, output_directory, generator_name, plot_name, plot_id, show_plot=False):\n",
    "    \"\"\"\n",
    "    Generates a plot a two-class dataset and saves it to a file.\n",
    "    \n",
    "    Args:\n",
    "        data (pandas.DataFrame): \n",
    "            Records along with their labels.\n",
    "        output_directory (str): \n",
    "            Name of the directory where images are saved.\n",
    "        generator_name (str): \n",
    "            Name of the applied recourse generator.\n",
    "        plot_name (str): \n",
    "            Type of the created plot.\n",
    "        plot_id (str): \n",
    "            ID for the generated plot (e.g. consecutive numbers for different distributions).\n",
    "        show_plot (Boolean): \n",
    "            If True the plot will also be outputted directly to the notebook.\n",
    "    \"\"\"\n",
    "    colors = np.array(['#1f77b4', '#ff7f0e'])\n",
    "    plt.scatter(data['feature1'], data['feature2'], c=colors[data['target'].astype(int)])\n",
    "    plt.axis('equal')\n",
    "    plt.grid(True)\n",
    "    plt.xlim([-0.2, 1.2])\n",
    "    plt.ylim([-0.3, 1.3])\n",
    "    plt.suptitle(f'Recourse generated by {generator_name.upper()} at t = {plot_id}')\n",
    "    plt.savefig(f'{output_directory}/{plot_name}_{plot_id}.png', bbox_inches='tight')\n",
    "    if show_plot:\n",
    "        plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Generalize code and names\n",
    "# TODO: Split this into more code blocks for readability (what would be a good way?)\n",
    "# TODO: What to do if a generator times out? do we accept different numbers of samples?\n",
    "def run_experiment(dataset, hyper_params, epochs=0.5, recourse_per_epoch=0.01,\n",
    "                   generator_timeout=120, generator_params={}, benchmark_params={}):\n",
    "    \"\"\"\n",
    "    Driver code to execute an experiment that allows to compare the dynamics of recourse \n",
    "    applied by some generator to a benchmark described by Wachter et al. (2017)\n",
    "    \n",
    "    Args:\n",
    "        dataset (DataCatalog): \n",
    "            Catalog containing a dataframe, set of train and test records, and the target.\n",
    "        hyper_params (dict): \n",
    "            Dictionary storing all custom hyper-parameter values for the model.\n",
    "        epochs (float): \n",
    "            Value between 0 and 1 representing the proportion of samples from the training set\n",
    "            which should have recourse applied to them throughout the experiment.\n",
    "        recourse_per_epoch (float): \n",
    "            Value between 0 and 1 representing the proportion of samples from the training set\n",
    "            which should have recourse applied to them in a single iteration.\n",
    "        generator_timeout (int): \n",
    "            Number of seconds after which the search for counterfactuals will time out.\n",
    "        generator_params (dict): \n",
    "            Dictionary representing the hyper-parameters for the generator under test (DICE).\n",
    "        benchmark_params (dict): \n",
    "            Dictionary representing the hyper-parameters for the benchmark generator (Wachter).\n",
    "    \"\"\"\n",
    "     \n",
    "    # Experiment data is saved into a new directory\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
    "    os.makedirs(f'../experiment_data/{timestamp}')\n",
    "    \n",
    "    # 1. Measure the initial distribution of data\n",
    "    experiment_data = {'dice': {0: {}}, 'wachter': {0: {}}}\n",
    "    experiment_data['dice'][0]['distribution'] = measure_distribution(dataset)\n",
    "    experiment_data['wachter'][0]['distribution'] = measure_distribution(dataset)\n",
    "    \n",
    "    # 2. Train classifiers C1, C2 on the dataset (one for DICE, one for benchmarking with Wachter)\n",
    "    log.info(\"Training the initial model\")\n",
    "    model = train_model(dataset, hyper_params)\n",
    "    \n",
    "    # Recourse generated by DICE is compared with the Wachter generator, as they may modify data differently\n",
    "    # we need to keep track of two models and two datasets and update them independently\n",
    "    benchmark_model = deepcopy(model)\n",
    "    benchmark_dataset = deepcopy(dataset)\n",
    "    \n",
    "    # 3. Measure the performance of the initial models\n",
    "    acc_dice, f1_dice = performance_metrics(dataset, model)\n",
    "    experiment_data['dice'][0][\"performance\"] = {\"acc\": acc_dice, \"f1\": f1_dice}\n",
    "    \n",
    "    acc_wachter, f1_wachter = performance_metrics(benchmark_dataset, benchmark_model)\n",
    "    experiment_data['wachter'][0][\"performance\"] = {\"acc\": acc_dice, \"f1\": f1_dice}\n",
    "     \n",
    "    # factuals are a list of instances that the model expects to belong to the negative class;\n",
    "    # in order to acurately measure the performance of the dataset we never change the test set\n",
    "    factuals = predict_negative_instances(model, dataset.df_train)\n",
    "    factuals_index = factuals.index.tolist()\n",
    "    \n",
    "    # TODO: make this slightly more robust\n",
    "    recourse_per_epoch = max(int(recourse_per_epoch * len(factuals)), 1)\n",
    "    epochs = max(int(min(epochs, 1) * len(factuals) / recourse_per_epoch), 1)\n",
    "\n",
    "    # Instantiate recourse generators\n",
    "    dice = Dice(model, generator_params)\n",
    "    wachter = Wachter(benchmark_model, benchmark_params)\n",
    "    \n",
    "    plot_distribution(dataset._df, f'../experiment_data/{timestamp}', \"dice\", \"distribution\", 0)\n",
    "    plot_distribution(benchmark_dataset._df, f'../experiment_data/{timestamp}', \"wachter\", \"distribution\", 0)\n",
    "\n",
    "    for epoch in range(epochs - 1):\n",
    "        log.info(f\"Starting epoch {epoch + 1}\")\n",
    "        experiment_data['dice'][epoch + 1] = {}\n",
    "        experiment_data['wachter'][epoch + 1] = {}\n",
    "        # 4. Generate a subset S of factuals that have never been encountered by the generators\n",
    "        current_factuals_index = np.random.choice(factuals_index, replace=False, size=recourse_per_epoch)\n",
    "        current_factuals = dataset._df.iloc[current_factuals_index]\n",
    "        # We do not want to accidentally generate a counterfactual from a counterfactual\n",
    "        factuals_index = [f for f in factuals_index if f not in current_factuals_index]  \n",
    "        \n",
    "        # 5. Apply recourse for S with DICE\n",
    "        log.info(f\"Applying DICE\")\n",
    "        dice_counterfactuals_found = 0\n",
    "        # We have to modify factuals one by one because CARLA outputs diverse counterfactuals \n",
    "        # generated for different factuals in a single dataframe\n",
    "        for i in range(len(current_factuals)):\n",
    "            f = current_factuals.iloc[[i]]\n",
    "            \n",
    "            log.info(f\"Generating counterfactual {i + 1} with DICE\")\n",
    "            # CARLA does not implement a timeout for the generators by default\n",
    "            # but we want to prevent the code from running indefinitely\n",
    "            dice_counterfactuals = recourse_controller(recourse_worker, generator_timeout, dice, f)\n",
    "            # We only want to overwrite the existing data if counterfactual generation was successful\n",
    "            if dice_counterfactuals is not None and not dice_counterfactuals.empty:\n",
    "                dataset._df.iloc[f.index[0]] = dice_counterfactuals.iloc[0]\n",
    "                dice_counterfactuals_found += 1\n",
    "        experiment_data['dice'][epoch + 1]['counterfactuals'] = dice_counterfactuals_found\n",
    "        \n",
    "        model.data._df.update(dataset._df)\n",
    "        model.data._df_train.update(dataset._df)\n",
    "        \n",
    "        # 6. Apply recourse for S with Wachter\n",
    "        log.info(f\"Applying the Wachter generator\")\n",
    "        wachter_counterfactuals = wachter.get_counterfactuals(current_factuals).dropna()\n",
    "        benchmark_dataset._df.update(wachter_counterfactuals)\n",
    "        experiment_data['wachter'][epoch + 1]['counterfactuals'] = len(wachter_counterfactuals.index)\n",
    "        \n",
    "        benchmark_model.data._df.update(benchmark_dataset._df)\n",
    "        benchmark_model.data._df_train.update(benchmark_dataset._df)\n",
    "\n",
    "        # 7. Measure the quality of recourse\n",
    "        \n",
    "        # 9. Retrain C1, C2 on the updated datasets\n",
    "        log.info(\"Updating the DICE model\")\n",
    "        model = train_model(dataset, hyper_params, retrain=True)\n",
    "        \n",
    "        log.info(\"Updating the benchmark model\")\n",
    "        benchmark_model = train_model(benchmark_dataset, hyper_params, retrain=True)\n",
    "        \n",
    "        # 8. Measure the resulting distributions of data\n",
    "        experiment_data['dice'][epoch + 1]['distribution'] = measure_distribution(dataset)\n",
    "        experiment_data['wachter'][epoch + 1]['distribution'] = measure_distribution(benchmark_dataset)\n",
    "        \n",
    "        # 10. Measure the performance of updated models\n",
    "        acc_dice, f1_dice = performance_metrics(dataset, model)\n",
    "        experiment_data['dice'][epoch + 1][\"performance\"] = {\"acc\": acc_dice, \"f1\": f1_dice}\n",
    "\n",
    "        acc_wachter, f1_wachter = performance_metrics(dataset, model)\n",
    "        experiment_data['wachter'][epoch + 1][\"performance\"] = {\"acc\": acc_dice, \"f1\": f1_dice}\n",
    "        \n",
    "        plot_distribution(dataset._df, f'../experiment_data/{timestamp}', \"dice\", \"distribution\", epoch + 1)\n",
    "        plot_distribution(benchmark_dataset._df, f'../experiment_data/{timestamp}', \"wachter\", \"distribution\", epoch + 1)\n",
    "    \n",
    "    # 11. Output results\n",
    "    with open(f\"../experiment_data/{timestamp}/measurements.json\", 'w') as outfile:\n",
    "        json.dump(experiment_data, outfile, sort_keys=True, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset from a csv file\n",
    "dataset = DynamicCsvCatalog(file_path=\"../datasets/unimodal_dataset_1.csv\", \n",
    "                            continuous=['feature1', 'feature2'], categorical=[],\n",
    "                            immutables=[], target='target', test_size=0.2)\n",
    "\n",
    "# Encoding of the classes applied in the dataset\n",
    "positive_class = 1\n",
    "negative_class = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyper_params = {'learning_rate': 0.04, 'epochs': 3, 'batch_size': 1, 'hidden_size': [4]}\n",
    "dice_params = {\"num\": 3}\n",
    "wachter_params = {\"loss_type\": \"BCE\", \"t_max_min\": 0.1}\n",
    "run_experiment(dataset, hyper_params, epochs=0.2, recourse_per_epoch=0.01,\n",
    "               generator_timeout=20, generator_params=dice_params, benchmark_params=wachter_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CSE3000",
   "language": "python",
   "name": "cse3000"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
