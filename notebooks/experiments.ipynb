{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", message=r\"Passing\", category=FutureWarning)\n",
    "warnings.filterwarnings(\"ignore\", message=r\"Implicit\", category=UserWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Using Python-MIP package version 1.12.0 [model.py <module>]\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "from carla import log\n",
    "from carla.models.negative_instances import predict_negative_instances\n",
    "from carla.recourse_methods import Dice, Wachter\n",
    "from copy import deepcopy\n",
    "from datetime import datetime\n",
    "from func_timeout import func_timeout, FunctionTimedOut\n",
    "from ipynb.fs.full.dynamic_recourse import DynamicCsvCatalog, DynamicMLModelCatalog\n",
    "from kneed import KneeLocator\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\"\n",
    "\n",
    "#     torch.manual_seed(0)\n",
    "#     random.seed(0)\n",
    "#     np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def k_means(data, min_clusters=1, max_clusters=10):\n",
    "    \"\"\"\n",
    "    Applies the k-means algorithm and automatically estimates the elbow point.\n",
    "    The algorithm used to calculate the elbow point is described in 10.1109/ICDCSW.2011.20\n",
    "    \n",
    "    Args:\n",
    "        data (pandas.DataFrame): \n",
    "            Records along with their labels.\n",
    "        min_clusters (int): \n",
    "            Minimal number of clusters that is expected in the dataset.\n",
    "        max_clusters (int):\n",
    "            Maximal number of clusters that is expected in the dataset.\n",
    "        \n",
    "    Returns:\n",
    "        int: Estimated number of clusters that yields the elbow point on an inertia graph.\n",
    "    \"\"\"\n",
    "    clusters = []\n",
    "    scores = []\n",
    "    # Fit different potential numbers of clusters\n",
    "    for k in range(min_clusters, max_clusters + 1):\n",
    "        kmeans = KMeans(n_clusters=k, random_state=42).fit(data)\n",
    "        clusters.append(k)\n",
    "        scores.append(kmeans.inertia_)   \n",
    "    \n",
    "    # Automatically find the elbow point, this should change at some point during the application of AR\n",
    "    # if the counterfactual instances form their own cluster(s), the value returned by this method should change.\n",
    "    kneedle = KneeLocator(clusters, scores, curve=\"convex\", direction=\"decreasing\")\n",
    "\n",
    "    return int(kneedle.elbow)\n",
    "\n",
    "def class_statistics(data, target, aggregate):\n",
    "    \"\"\"\n",
    "    Applies an aggregation function for the two classes described in the dataset.\n",
    "    \n",
    "    Args:\n",
    "        data (pandas.DataFrame):\n",
    "            Records along with their labels.\n",
    "        target (str): \n",
    "            Name of the target column.\n",
    "        aggregate (Callable): \n",
    "            An aggregation function which can be applied on data.\n",
    "        \n",
    "    Returns:\n",
    "        dict: Values return by the aggregation function applied on the positive and negative class.\n",
    "    \"\"\"\n",
    "    features = data.loc[:, data.columns != target]\n",
    "    positive_samples = features.loc[data[target] == positive_class]\n",
    "    negative_samples = features.loc[data[target] == negative_class]\n",
    "    return {\"positive\": aggregate(positive_samples).to_dict(), \n",
    "            \"negative\": aggregate(negative_samples).to_dict()}\n",
    "\n",
    "def measure_distribution(dataset):\n",
    "    \"\"\"\n",
    "    Computes a set of statistical measures for the distribution of a dataset.\n",
    "    \n",
    "    Args:\n",
    "        dataset (DataCatalog):\n",
    "            Catalog containing a dataframe, set of train and test records, and the target.\n",
    "        \n",
    "    Returns:\n",
    "        dict: Statistics calculated for a specific distribution of data.\n",
    "    \"\"\"\n",
    "    num_clusters = k_means(dataset.df.loc[:, dataset.df.columns != dataset.target].to_numpy())\n",
    "    means = class_statistics(dataset.df, dataset.target, np.mean)\n",
    "    stds = class_statistics(dataset.df, dataset.target, np.std)\n",
    "    \n",
    "    return {\"num_clusters\": num_clusters, \"means\": means, \"stds\": stds}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(dataset, hyper_params, model_type='ann', retrain=False):\n",
    "    \"\"\"\n",
    "    Instantiates and trains a black-box model within a CARLA wrapper that will be used to generate explanations.\n",
    "    \n",
    "    Args:\n",
    "        dataset (DataCatalog): \n",
    "            Catalog containing a dataframe, set of train and test records, and the target.\n",
    "        hyper_params (dict): \n",
    "            Dictionary storing all custom hyper-parameter values for the model.\n",
    "        \n",
    "    Returns:\n",
    "        MLModelCatalog: \n",
    "            Classifier with additional utilities required by CARLA.\n",
    "    \"\"\"\n",
    "    kwargs = {'save_name_params': \"_\".join([str(size) for size in hyper_params['hidden_size']])}\n",
    "    model = DynamicMLModelCatalog(dataset, model_type=model_type,\n",
    "                                  load_online=False, backend=\"pytorch\", **kwargs)\n",
    "\n",
    "    # force_train is enabled to ensure that the model is not reused from the cache\n",
    "    if not retrain:\n",
    "        log.info(\"Training the initial model\")\n",
    "        model.train(learning_rate=hyper_params['learning_rate'],\n",
    "                    epochs=hyper_params['epochs'],\n",
    "                    batch_size=hyper_params['batch_size'],\n",
    "                    hidden_size=hyper_params['hidden_size'],\n",
    "                    force_train=True)\n",
    "    else:\n",
    "        model.retrain(learning_rate=hyper_params['learning_rate'],\n",
    "                      epochs=hyper_params['epochs'],\n",
    "                      batch_size=hyper_params['batch_size'])\n",
    "        \n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def performance(dataset, model):\n",
    "    \"\"\"\n",
    "    Computes a set of performance metrics for a classifier.\n",
    "    \n",
    "    Args:\n",
    "        dataset (DataCatalog): \n",
    "            Catalog containing a dataframe, set of train and test records, and the target.\n",
    "        model (MLModelCatalog) \n",
    "            Classifier with additional utilities required by CARLA.\n",
    "    \n",
    "    Returns:\n",
    "        dict: A dictionary of statistical measurements of the model performance.\n",
    "    \"\"\"\n",
    "    predictions = model.predict_proba(dataset.df_test.loc[:, dataset.df_test.columns != dataset.target])\n",
    "    predicted_labels = np.argmax(predictions, axis=1) \n",
    "    ground_truth = dataset.df_test.loc[:, dataset.df_test.columns == dataset.target].values.astype(int).flatten() \n",
    "    \n",
    "    return {\n",
    "        \"acc\": accuracy_score(ground_truth, predicted_labels),\n",
    "        \"f1\": f1_score(ground_truth, predicted_labels)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recourse_worker(generator, factual):\n",
    "    \"\"\"\n",
    "    Apply algorithmic recourse for a (set of) factuals using a chosen generator.\n",
    "    \n",
    "    Args:\n",
    "        generator (RecourseMethod): \n",
    "            Generator that finds counterfactual explanations using a black-box model.\n",
    "        factual (pandas.DataFrame): \n",
    "            One or more records from a dataset used to train the black-box model.\n",
    "        \n",
    "    Returns:\n",
    "        pandas.DataFrame: A counterfactual explanation for the provided factual.\n",
    "    \"\"\"\n",
    "    if factual is None:\n",
    "        raise ValueException('Provided with a non-existent factual')\n",
    "        return None\n",
    "    \n",
    "    counterfactuals = generator.get_counterfactuals(factual).dropna()\n",
    "    if not counterfactuals.empty:\n",
    "        return counterfactuals.sample().astype(float)\n",
    "    raise FunctionTimedOut()\n",
    "    \n",
    "def recourse_controller(function, max_wait_time, generator, factual):\n",
    "    \"\"\"\n",
    "    Wrapper function that ensures the application of recourse does not run indefinitely.\n",
    "    \n",
    "    Args:\n",
    "        function (Callable): \n",
    "            Function that will have its execution placed under a timeout.\n",
    "        max_wait_time (int): \n",
    "            Number of seconds after which the `function` will time out.\n",
    "        generator (RecourseMethod): \n",
    "            Generator that finds counterfactual explanations using a black-box model.\n",
    "        factual (pandas.DataFrame): \n",
    "            One or more records from a dataset used to train the black-box model.\n",
    "        \n",
    "    Returns:\n",
    "        pandas.DataFrame: A counterfactual explanation for the provided factual if found.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        return func_timeout(max_wait_time, function, args=[generator, factual]) \n",
    "    except FunctionTimedOut:\n",
    "        log.info(\"Timeout - No Counterfactual Explanation Found\")\n",
    "\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_distribution(data, output_directory, generator_name, plot_name, plot_id, show_plot=False):\n",
    "    \"\"\"\n",
    "    Generates a plot a two-class dataset and saves it to a file.\n",
    "    \n",
    "    Args:\n",
    "        data (pandas.DataFrame): \n",
    "            Records along with their labels.\n",
    "        output_directory (str): \n",
    "            Name of the directory where images are saved.\n",
    "        generator_name (str): \n",
    "            Name of the applied recourse generator.\n",
    "        plot_name (str): \n",
    "            Type of the created plot.\n",
    "        plot_id (str): \n",
    "            ID for the generated plot (e.g. consecutive numbers for different distributions).\n",
    "        show_plot (Boolean): \n",
    "            If True the plot will also be outputted directly to the notebook.\n",
    "    \"\"\"\n",
    "    colors = np.array(['#1f77b4', '#ff7f0e'])\n",
    "    plt.scatter(data['feature1'], data['feature2'], c=colors[data['target'].astype(int)])\n",
    "    \n",
    "    plt.axis('equal')\n",
    "    plt.grid(True)\n",
    "    plt.xlim([-0.2, 1.2])\n",
    "    plt.ylim([-0.3, 1.3])\n",
    "    \n",
    "    plt.suptitle(f'Recourse generated by {generator_name.upper()} at t = {plot_id}')\n",
    "    plt.savefig(f\"{output_directory}/{generator_name}_{plot_name}_{f'{plot_id:02}'}.png\", bbox_inches='tight')\n",
    "    \n",
    "    if show_plot:\n",
    "        plt.show()\n",
    "        \n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Generalize for other recourse generators\n",
    "# TODO: What to do if a generator times out? do we accept different numbers of samples?\n",
    "class RecourseExperiment():\n",
    "    \"\"\"\n",
    "    Allows to conduct an experiment about the dynamics of algorithmic recourse.\n",
    "    \n",
    "    Attributes:\n",
    "        dataset (DataCatalog): \n",
    "            Catalog containing a dataframe, set of train and test records, and the target.\n",
    "        generator_name (str):\n",
    "            Name of the generator that is placed under test.\n",
    "        generator_timeout (int): \n",
    "            Number of seconds after which the search for counterfactuals will time out.\n",
    "        experiment_name (str):\n",
    "            Name of the experiment that will be used as part of the directory name where results are saved.\n",
    "        kwargs (dict):\n",
    "            Dictionary of optional keyworded arguments.  \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, dataset, total_recourse=0.5, recourse_per_epoch=0.01, generator_name='DICE',\n",
    "                 generator_timeout=120, experiment_name='experiment', **kwargs):\n",
    "    \n",
    "        # Experiment data is saved into a new directory\n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
    "        self.experiment_name = f'{timestamp}_{experiment_name}'\n",
    "        os.makedirs(f'../experiment_data/{self.experiment_name}')\n",
    "        self.generator_name = generator_name\n",
    "        self.generator_timeout = generator_timeout\n",
    "\n",
    "        self.experiment_data = {\n",
    "            self.generator_name: {0: {}},\n",
    "            'wachter': {0: {}}\n",
    "        }\n",
    "\n",
    "        # Train a classifier on the dataset\n",
    "        self.hyper_params = kwargs.get('hyper_params',\n",
    "                                       {'learning_rate': 0.04, 'epochs': 3, 'batch_size': 1, 'hidden_size': [4]})\n",
    "        self.dataset = dataset\n",
    "        self.model = train_model(dataset, self.hyper_params)\n",
    "\n",
    "        # Recourse generated by DICE is compared with the Wachter generator, as they may modify data differently\n",
    "        # we need to keep track of two models and two datasets and update them independently\n",
    "        self.benchmark_dataset = deepcopy(dataset)\n",
    "        self.benchmark_model = deepcopy(self.model)\n",
    "\n",
    "        # factuals are a list of instances that the model expects to belong to the negative class;\n",
    "        # in order to acurately measure the performance of the dataset we never change the test set\n",
    "        self.factuals = predict_negative_instances(self.model, dataset.df_train)\n",
    "        self.factuals_index = self.factuals.index.tolist()\n",
    "\n",
    "        # Instantiate recourse generators\n",
    "        self.generator = Dice(self.model, kwargs.get('generator_params', {\"num\": 5}))\n",
    "        default_benchmark_params = {\"loss_type\": \"BCE\", \"t_max_min\": generator_timeout / 60}\n",
    "        self.benchmark = Wachter(self.benchmark_model, kwargs.get('benchmark_params', default_benchmark_params))\n",
    "                              \n",
    "    def run(self, total_recourse=0.8, recourse_per_epoch=0.01):\n",
    "        \"\"\"\n",
    "        Driver code to execute an experiment that allows to compare the dynamics of recourse \n",
    "        applied by some generator to a benchmark described by Wachter et al. (2017).\n",
    "        \n",
    "        Attributes:\n",
    "            total_recourse (float): \n",
    "                Value between 0 and 1 representing the proportion of samples from the training set\n",
    "                which should have recourse applied to them throughout the experiment.\n",
    "            recourse_per_epoch (float): \n",
    "                Value between 0 and 1 representing the proportion of samples from the training set\n",
    "                which should have recourse applied to them in a single iteration.\n",
    "        \"\"\"\n",
    "    \n",
    "        num_found_generator = 0\n",
    "        num_found_benchmark = 0\n",
    "           \n",
    "        # Measure the data distribution and performance of models\n",
    "        self.measure(0)\n",
    "        \n",
    "        # Plot initial data distributions\n",
    "        path = f'../experiment_data/{self.experiment_name}'\n",
    "        plot_distribution(self.dataset._df, path, self.generator_name, 'distribution', 0)\n",
    "        plot_distribution(self.benchmark_dataset._df, path, 'wachter', 'distribution', 0)\n",
    "        \n",
    "        # Convert ratio of samples that should undergo recourse in a single epoch into a number\n",
    "        recourse_per_epoch = max(int(recourse_per_epoch * len(self.factuals)), 1)\n",
    "        # Convert ratio of samples that should undergo recourse in total into a number of epochs\n",
    "        epochs = max(int(min(total_recourse, 1) * len(self.factuals) / recourse_per_epoch), 1)\n",
    "                              \n",
    "        for epoch in range(epochs - 1):\n",
    "            log.info(f\"Starting epoch {epoch + 1}\")\n",
    "            self.experiment_data[self.generator_name][epoch + 1] = {}\n",
    "            self.experiment_data['wachter'][epoch + 1] = {}\n",
    "                              \n",
    "            # 4. Generate a subset S of factuals that have never been encountered by the generators\n",
    "            sample_size = min(recourse_per_epoch, len(self.factuals_index))\n",
    "            current_factuals_index = np.random.choice(self.factuals_index, replace=False, size=sample_size)\n",
    "            current_test_factuals = self.dataset._df.iloc[current_factuals_index]\n",
    "            current_benchmark_factuals = self.benchmark_dataset._df.iloc[current_factuals_index] \n",
    "                              \n",
    "            # We do not want to accidentally generate a counterfactual from a counterfactual\n",
    "            self.factuals_index = [f for f in self.factuals_index if f not in current_factuals_index]  \n",
    "\n",
    "            # Apply recourse for S with DICE\n",
    "            num_found_generator += self.apply_recourse_with_timeout(self.dataset,\n",
    "                                                                    current_test_factuals,\n",
    "                                                                    self.generator)\n",
    "\n",
    "            # Update DICE model\n",
    "            self.model = self.update_model(self.dataset, self.model, self.generator_name)\n",
    "\n",
    "\n",
    "            # Apply recourse for S with Wachter\n",
    "            num_found_benchmark += self.apply_recourse(self.benchmark_dataset,\n",
    "                                                       current_benchmark_factuals,\n",
    "                                                       self.benchmark)\n",
    "            \n",
    "            # Update the Wachter model\n",
    "            self.benchmark_model = self.update_model(self.benchmark_dataset, self.benchmark_model, 'Wachter')\n",
    "                           \n",
    "            # Measure the data distribution and performance of models\n",
    "            self.measure(epoch + 1)\n",
    "\n",
    "            # Plot initial data distributions\n",
    "            plot_distribution(self.dataset._df, path, self.generator_name, 'distribution', epoch + 1)\n",
    "            plot_distribution(self.benchmark_dataset._df, path, 'wachter', 'distribution', epoch + 1)\n",
    "            \n",
    "            # Measure the quality of recourse\n",
    "            self.experiment_data['evaluation'] = {\n",
    "                'dice': {\n",
    "                    'success_rate': num_found_generator / max(len(self.factuals.index) - len(self.factuals_index), 1)\n",
    "                },\n",
    "                'wachter': {\n",
    "                    'success_rate': num_found_benchmark / max(len(self.factuals.index) - len(self.factuals_index), 1)\n",
    "                }\n",
    "            }                  \n",
    "  \n",
    "\n",
    "    def measure(self, epoch):\n",
    "        \"\"\"\n",
    "        Quantify the dataset and model and save into `experiment_data`.\n",
    "        \n",
    "        Args:\n",
    "            epoch (int): \n",
    "                Current epoch in the experiment.\n",
    "        \"\"\"     \n",
    "        # Measure the distributions of data\n",
    "        self.experiment_data[self.generator_name][epoch]['distribution'] = measure_distribution(self.dataset)\n",
    "        self.experiment_data['wachter'][epoch]['distribution'] = measure_distribution(self.benchmark_dataset)\n",
    "\n",
    "        # Measure the current performance of models\n",
    "        self.experiment_data[self.generator_name][epoch][\"performance\"] = performance(self.dataset,\n",
    "                                                                         self.model)\n",
    "        self.experiment_data['wachter'][epoch][\"performance\"] = performance(self.benchmark_dataset,\n",
    "                                                                            self.benchmark_model)     \n",
    "                              \n",
    "    def update_model(self, dataset, model, generator_name):\n",
    "        \"\"\"\n",
    "        Re-train the model based on an updated dataset\n",
    "        \n",
    "        Args:\n",
    "            dataset (DataCatalog): \n",
    "                Catalog containing a dataframe, set of train and test records, and the target.\n",
    "            model (MLModelCatalog) \n",
    "                Classifier with additional utilities required by CARLA.\n",
    "            generator_name (str):\n",
    "                Name of the generator that has its model updated.\n",
    "                \n",
    "        Returns:\n",
    "            MLModelCatalog: A re-trained classifier.\n",
    "        \"\"\"\n",
    "        # Ensure that the dataset saved by the model is always updated with counterfactuals\n",
    "        model.data._df.update(dataset._df)\n",
    "        model.data._df_train.update(dataset._df)\n",
    "                              \n",
    "        log.info(f'Updating the {generator_name} model')                      \n",
    "        return train_model(dataset, hyper_params, retrain=True)                    \n",
    "        \n",
    "                                    \n",
    "    def apply_recourse(self, dataset, factuals, generator, generator_name='Wachter'):\n",
    "        \"\"\"\n",
    "        Generate (a set of) counterfactual explanations with a provided generator.\n",
    "        \n",
    "        Args:\n",
    "            dataset (DataCatalog): \n",
    "                Catalog containing a dataframe, set of train and test records, and the target.\n",
    "            factuals (pandas.DataFrame): \n",
    "                One or more records from a dataset used to train the black-box model.\n",
    "            generator (RecourseMethod): \n",
    "                Generator that finds counterfactual explanations using a black-box model.\n",
    "            generator_name (str):\n",
    "                Name of the generator that is used to apply recourse.\n",
    "                \n",
    "        Returns:\n",
    "            int: Number of successfully generated counterfactuals.\n",
    "        \"\"\"\n",
    "        log.info(f\"Applying the {generator_name} generator.\")\n",
    "        \n",
    "        if factuals is None or len(factuals) == 0:\n",
    "            return 0\n",
    "                                              \n",
    "        counterfactuals = generator.get_counterfactuals(factuals).dropna()\n",
    "        dataset._df.update(counterfactuals)\n",
    "        \n",
    "        return len(counterfactuals.index)\n",
    "  \n",
    "                              \n",
    "    def apply_recourse_with_timeout(self, dataset, factuals, generator, generator_name='DICE'):\n",
    "        \"\"\"\n",
    "        Generate (a set of) counterfactual explanations with a provided generator. \n",
    "        These explanations are applied one-by-one with a specific timeout for every single factual.\n",
    "        \n",
    "        Args:\n",
    "            dataset (DataCatalog): \n",
    "                Catalog containing a dataframe, set of train and test records, and the target.\n",
    "            factuals (pandas.DataFrame): \n",
    "                One or more records from a dataset used to train the black-box model.\n",
    "            generator (RecourseMethod): \n",
    "                Generator that finds counterfactual explanations using a black-box model.\n",
    "            generator_name (str):\n",
    "                Name of the generator that is used to apply recourse.\n",
    "                \n",
    "        Returns:\n",
    "            int: Number of successfully generated counterfactuals.\n",
    "        \"\"\"\n",
    "        log.info(f\"Applying the {generator_name} generator.\")\n",
    "        \n",
    "        if factuals is None or len(factuals) == 0:\n",
    "            return 0\n",
    "                              \n",
    "        counterfactuals_found = 0\n",
    "        for i in range(len(factuals)):\n",
    "            f = factuals.iloc[[i]]\n",
    "            \n",
    "            log.info(f\"Generating counterfactual {i + 1} with {generator_name}\")\n",
    "            # CARLA does not implement a timeout for the generators by default\n",
    "            # but we want to prevent the code from running indefinitely\n",
    "            counterfactuals = recourse_controller(recourse_worker, self.generator_timeout, generator, f)\n",
    "            # We only want to overwrite the existing data if counterfactual generation was successful\n",
    "            if counterfactuals is not None and not counterfactuals.empty:\n",
    "                dataset._df.iloc[f.index[0]] = counterfactuals.iloc[0]\n",
    "                counterfactuals_found += 1\n",
    "                         \n",
    "        return counterfactuals_found\n",
    "                \n",
    "                         \n",
    "    def save_data(self, path=None):\n",
    "        \"\"\"\n",
    "        Write the data collected throughout the experiment into a .json file.\n",
    "        \"\"\"\n",
    "        \n",
    "        path = path or f'../experiment_data/{self.experiment_name}/measurements.json'\n",
    "        with open(path, 'w') as outfile:\n",
    "            json.dump(self.experiment_data, outfile, sort_keys=True, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset from a csv file\n",
    "dataset = DynamicCsvCatalog(file_path=\"../datasets/unimodal_dataset_1.csv\", \n",
    "                            continuous=['feature1', 'feature2'], categorical=[],\n",
    "                            immutables=[], target='target', test_size=0.2)\n",
    "\n",
    "# Encoding of the classes applied in the dataset\n",
    "positive_class = 1\n",
    "negative_class = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Training the initial model [3473874465.py train_model]\n",
      "balance on test set 0.4875, balance on test set 0.55\n",
      "Epoch 0/2\n",
      "----------\n",
      "train Loss: 0.3494 Acc: 0.8375\n",
      "\n",
      "test Loss: 0.0805 Acc: 0.9750\n",
      "\n",
      "Epoch 1/2\n",
      "----------\n",
      "train Loss: 0.0341 Acc: 1.0000\n",
      "\n",
      "test Loss: 0.0230 Acc: 1.0000\n",
      "\n",
      "Epoch 2/2\n",
      "----------\n",
      "train Loss: 0.0129 Acc: 1.0000\n",
      "\n",
      "test Loss: 0.0073 Acc: 1.0000\n",
      "\n",
      "[INFO] Starting epoch 1 [2388305276.py run]\n",
      "[INFO] Applying the DICE generator. [2388305276.py apply_recourse_with_timeout]\n",
      "[INFO] Generating counterfactual 1 with DICE [2388305276.py apply_recourse_with_timeout]\n",
      "[INFO] Timeout - No Counterfactual Explanation Found [3910937888.py recourse_controller]\n",
      "[INFO] Generating counterfactual 2 with DICE [2388305276.py apply_recourse_with_timeout]\n",
      "[INFO] Generating counterfactual 3 with DICE [2388305276.py apply_recourse_with_timeout]\n"
     ]
    }
   ],
   "source": [
    "hyper_params = {'learning_rate': 0.04, 'epochs': 3, 'batch_size': 1, 'hidden_size': [4]}\n",
    "dice_params = {\"num\": 3}\n",
    "wachter_params = {\"loss_type\": \"BCE\", \"t_max_min\": 0.1}\n",
    "\n",
    "kwargs = {\n",
    "    'hyper_params': hyper_params,\n",
    "    'dice_params': dice_params,\n",
    "    'wachter_params': wachter_params\n",
    "}\n",
    "\n",
    "experiment = RecourseExperiment(dataset, experiment_name='uniform', **kwargs)\n",
    "experiment.run(total_recourse=0.2, recourse_per_epoch=0.05)\n",
    "experiment.save_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CSE3000",
   "language": "python",
   "name": "cse3000"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
