{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f608fd63",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", message=r\"Passing\", category=FutureWarning)\n",
    "warnings.filterwarnings(\"ignore\", message=r\"Implicit\", category=UserWarning)\n",
    "warnings.filterwarnings(\"ignore\", message=r\"cannot\", category=UserWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "24d0e397",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Using Python-MIP package version 1.12.0 [model.py <module>]\n"
     ]
    }
   ],
   "source": [
    "import carla.models.catalog.load_model as loading_utils\n",
    "import carla.models.catalog.train_model as training_utils\n",
    "import json\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import sys\n",
    "import timeit\n",
    "\n",
    "from carla import log, MLModelCatalog\n",
    "from carla.data.catalog import DataCatalog\n",
    "from carla.evaluation.benchmark import Benchmark\n",
    "from carla.models.negative_instances import predict_negative_instances\n",
    "from carla.recourse_methods import Dice, Wachter\n",
    "from copy import deepcopy\n",
    "from datetime import datetime\n",
    "from func_timeout import func_timeout, FunctionTimedOut\n",
    "from inspect import signature\n",
    "from ipynb.fs.full.metrics import current_MMD, disagreement_distance, measure_distribution, performance, boundary_distance\n",
    "from ipynb.fs.full.plotting import plot_distribution\n",
    "from sklearn.model_selection import train_test_split\n",
    "from typing import List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1ca8213e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DynamicCsvCatalog(DataCatalog):\n",
    "    \"\"\"\n",
    "    Wrapper class for the DataCatalog similar to the built-in CsvCatalog but with new capabilities\n",
    "    required to control data in the experiments.\n",
    "    \n",
    "    Attributes:\n",
    "        file_path (str): \n",
    "            Path to the .csv file containing the dataset.\n",
    "        categorical (List[str]): \n",
    "            Names of columns describing categorical features.\n",
    "        continuous (List[str]):\n",
    "            Names of columns describing continuous (i.e. numerical) features.\n",
    "        immutables (List[str]):\n",
    "            Names of columns describing immutable features, not supported by all generators.\n",
    "        target (str):\n",
    "            Name of the column that contains the target variable.\n",
    "        test_size (float):\n",
    "            Proportion of the dataset which should be withheld as an independent test set.\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    def __init__(self, file_path: str, categorical: List[str],  continuous: List[str],\n",
    "                 immutables: List[str], target: str, test_size: float = 0.5,\n",
    "                 scaling_method: str = \"MinMax\", encoding_method: str = \"OneHot_drop_binary\",\n",
    "                 positive=1, negative=0):\n",
    "        \n",
    "        self._categorical = categorical\n",
    "        self._continuous = continuous\n",
    "        self._immutables = immutables\n",
    "        self._target = target\n",
    "        self._positive = positive\n",
    "        self._negative = negative\n",
    "\n",
    "        # Load the raw data\n",
    "        raw = pd.read_csv(file_path)\n",
    "        train_raw, test_raw = train_test_split(raw, test_size=test_size, stratify=raw[target])\n",
    "\n",
    "        super().__init__(\"custom\", raw, train_raw, test_raw,\n",
    "                         scaling_method, encoding_method)\n",
    "\n",
    "    @property\n",
    "    def categorical(self) -> List[str]:\n",
    "        return self._categorical\n",
    "\n",
    "    @property\n",
    "    def continuous(self) -> List[str]:\n",
    "        return self._continuous\n",
    "\n",
    "    @property\n",
    "    def immutables(self) -> List[str]:\n",
    "        return self._immutables\n",
    "\n",
    "    @property\n",
    "    def target(self) -> str:\n",
    "        return self._target\n",
    "    \n",
    "    @property\n",
    "    def positive(self) -> str:\n",
    "        return self._positive\n",
    "    \n",
    "    @property\n",
    "    def negative(self) -> str:\n",
    "        return self._negative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7c73de88",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DynamicMLModelCatalog(MLModelCatalog):\n",
    "    \"\"\"\n",
    "    Wrapper class for the MLModelCatalog that introduces additional functions\n",
    "    allowing for the efficient and unbiased measurement of the dynamics of recourse.\n",
    "    \n",
    "    Attributes:\n",
    "        data (DataCatalog):\n",
    "            Dataset which will be used to train a model and conduct experiments.\n",
    "        model_type (str):\n",
    "            Black-box model used for classification, currently this class supports only ANNs and Logistic Regression.\n",
    "        backend (str):\n",
    "            Specifies the framework used on the backend, currently this class supports only PyTorch.\n",
    "        cache (Boolean):\n",
    "            If True, the framework will attempt to load a model that was previously cached.\n",
    "        models_home (str):\n",
    "            Path to the directory where models should be saved after they are trained.\n",
    "        load_online: (Boolean):\n",
    "            If True, a pretrained model will be loaded.\n",
    "        kwargs (dict):\n",
    "            Dictionary of optional keyworded arguments.\n",
    "    \"\"\"\n",
    "    def __init__(self, data: DataCatalog, model_type: str, backend: str = \"pytorch\",\n",
    "        cache: bool = True, models_home: str = None, load_online: bool = True, **kwargs) -> None:\n",
    "        \n",
    "        if backend != 'pytorch':\n",
    "            raise NotImplementedError(f\"Only PyTorch models are currently supported\")\n",
    "            \n",
    "        if model_type not in ['ann', 'linear']:\n",
    "            raise NotImplementedError(f\"Model type not supported: {self.model_type}\")\n",
    "            \n",
    "        save_name = model_type\n",
    "        if model_type == \"ann\":\n",
    "            save_name += f\"_layers_{kwargs['save_name_params']}\"\n",
    "            \n",
    "        self._save_name = save_name\n",
    "        \n",
    "        super().__init__(data, model_type, backend, cache,\n",
    "                         models_home, load_online, **kwargs)\n",
    "    \n",
    "    @property\n",
    "    def save_name(self) -> str:\n",
    "        return self._save_name\n",
    "        \n",
    "        \n",
    "    def retrain(self, learning_rate=0.01, epochs=5, batch_size=1, hidden_size=[4]):\n",
    "        \"\"\"\n",
    "        Loads a cached model and retrains it on an updated dataset.\n",
    "        \n",
    "        Args:\n",
    "            learning_rate (float):\n",
    "                Size of the step at each epoch of the model training.\n",
    "            epochs (int):\n",
    "                Number of iterations of training.\n",
    "            batch_size (int):\n",
    "                Number of samples used at once in a gradient descent step, if '1' the procedure is stochastic.    \n",
    "        \"\"\"\n",
    "\n",
    "        # Attempt to load the saved model\n",
    "        self._model = loading_utils.load_trained_model(save_name=self._save_name,\n",
    "                                                       data_name=self.data.name,\n",
    "                                                       backend=self.backend)\n",
    "        \n",
    "        # This method should only be used when a model is already available\n",
    "        if self._model is None:\n",
    "            raise ValueError(f\"No trained model found for {self._save_name}\")\n",
    "        \n",
    "        # Sanity check to see if loaded model accuracy makes sense\n",
    "        if self._model is not None:\n",
    "            self._test_accuracy()\n",
    "            \n",
    "        # Get preprocessed data\n",
    "        df_train = self.data.df_train\n",
    "        df_test = self.data.df_test\n",
    "\n",
    "        # All dataframes may have possibly changed\n",
    "        x_train = df_train[list(set(df_train.columns) - {self.data.target})]\n",
    "        y_train = df_train[self.data.target]\n",
    "        x_test = df_test[list(set(df_test.columns) - {self.data.target})]\n",
    "        y_test = df_test[self.data.target]\n",
    "\n",
    "        # Order data (column-wise) before training\n",
    "        x_train = self.get_ordered_features(x_train)\n",
    "        x_test = self.get_ordered_features(x_test)\n",
    "        \n",
    "        log.info(f\"Current balance: train set {y_train.mean()}, test set {y_test.mean()}\")\n",
    "        \n",
    "        # Access the data in a format expected by PyTorch\n",
    "        train_dataset = training_utils.DataFrameDataset(x_train, y_train)\n",
    "        train_loader = training_utils.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "        test_dataset = training_utils.DataFrameDataset(x_test, y_test)\n",
    "        test_loader = training_utils.DataLoader(test_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "        # Retrain the model\n",
    "        training_utils._training_torch(self._model, train_loader, test_loader,\n",
    "                                       learning_rate, epochs)\n",
    "\n",
    "        loading_utils.save_model(model=self._model, save_name=self._save_name,\n",
    "                                 data_name=self.data.name, backend=self.backend)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6c58e5da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(dataset, hyper_params, model_type='ann', retrain=False):\n",
    "    \"\"\"\n",
    "    Instantiates and trains a black-box model within a CARLA wrapper that will be used to generate explanations.\n",
    "    \n",
    "    Args:\n",
    "        dataset (DataCatalog): \n",
    "            Catalog containing a dataframe, set of train and test records, and the target.\n",
    "        hyper_params (dict): \n",
    "            Dictionary storing all custom hyper-parameter values for the model.\n",
    "        \n",
    "    Returns:\n",
    "        MLModelCatalog: \n",
    "            Classifier with additional utilities required by CARLA.\n",
    "    \"\"\"\n",
    "    block_print()\n",
    "    kwargs = {'save_name_params': \"_\".join([str(size) for size in hyper_params['hidden_size']])}\n",
    "    model = DynamicMLModelCatalog(dataset, model_type=model_type,\n",
    "                                  load_online=False, backend=\"pytorch\", **kwargs)\n",
    "\n",
    "    # force_train is enabled to ensure that the model is not reused from the cache\n",
    "    if not retrain:\n",
    "        log.info(\"Training the initial model\")\n",
    "        model.train(learning_rate=hyper_params['learning_rate'],\n",
    "                    epochs=hyper_params['epochs'],\n",
    "                    batch_size=hyper_params['batch_size'],\n",
    "                    hidden_size=hyper_params['hidden_size'],\n",
    "                    force_train=True)\n",
    "    else:\n",
    "        model.retrain(learning_rate=hyper_params['learning_rate'],\n",
    "                      epochs=hyper_params['epochs'],\n",
    "                      batch_size=hyper_params['batch_size'])\n",
    "    \n",
    "    enable_print()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dc4e83a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def recourse_worker(generator, factual):\n",
    "    \"\"\"\n",
    "    Apply algorithmic recourse for a (set of) factuals using a chosen generator.\n",
    "    \n",
    "    Args:\n",
    "        generator (RecourseMethod): \n",
    "            Generator that finds counterfactual explanations using a black-box model.\n",
    "        factual (pandas.DataFrame): \n",
    "            One or more records from a dataset used to train the black-box model.\n",
    "        \n",
    "    Returns:\n",
    "        pandas.DataFrame: A counterfactual explanation for the provided factual.\n",
    "    \"\"\"\n",
    "    if factual is None:\n",
    "        raise ValueException('Provided with a non-existent factual')\n",
    "        return None\n",
    "    \n",
    "    counterfactuals = generator.get_counterfactuals(factual).dropna()\n",
    "    if not counterfactuals.empty:\n",
    "        return counterfactuals.sample().astype(float)\n",
    "    raise FunctionTimedOut()\n",
    "    \n",
    "def recourse_controller(function, max_wait_time, generator, factual):\n",
    "    \"\"\"\n",
    "    Wrapper function that ensures the application of recourse does not run indefinitely.\n",
    "    \n",
    "    Args:\n",
    "        function (Callable): \n",
    "            Function that will have its execution placed under a timeout.\n",
    "        max_wait_time (int): \n",
    "            Number of seconds after which the `function` will time out.\n",
    "        generator (RecourseMethod): \n",
    "            Generator that finds counterfactual explanations using a black-box model.\n",
    "        factual (pandas.DataFrame): \n",
    "            One or more records from a dataset used to train the black-box model.\n",
    "        \n",
    "    Returns:\n",
    "        pandas.DataFrame: A counterfactual explanation for the provided factual if found.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        return func_timeout(max_wait_time, function, args=[generator, factual]) \n",
    "    except FunctionTimedOut:\n",
    "        log.info(\"Timeout - No Counterfactual Explanation Found\")\n",
    "\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ecc31ee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RecourseGenerator():\n",
    "    \"\"\"\n",
    "    Wrapper class for the CARLA generators that contains utilities required to conduct experiments.\n",
    "    \n",
    "    Attributes:\n",
    "        name (str):\n",
    "            Name of the generator.\n",
    "        dataset (DataCatalog): \n",
    "            Catalog containing a dataframe, set of train and test records, and the target.\n",
    "        model (MLModelCatalog):\n",
    "            Classifier with additional utilities required by CARLA.\n",
    "        recourse_method (RecourseMethod):\n",
    "            A generator of algorithmic recourse implemented in CARLA.\n",
    "        generator_params (dict):\n",
    "            Dictionary of parameters that affect the actions of the generator.\n",
    "        model_params (dict):\n",
    "            Hyper-parameters for the underlying black-box model.\n",
    "        timeout (int):\n",
    "            Number of seconds after which the generation of a counterfactual should be considered a failure.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, name, dataset, model, recourse_method,\n",
    "                 generator_params, model_params, timeout=None):\n",
    "        self.name = name\n",
    "        self.dataset = dataset\n",
    "        self.model = model\n",
    "        self.recourse_method = recourse_method\n",
    "        self.generator_params = generator_params\n",
    "        self.model_params = model_params\n",
    "        self.timeout = timeout\n",
    "        self.num_found = 0\n",
    "        \n",
    "        self.update_generator()\n",
    "        \n",
    "        \n",
    "    def update_generator(self):\n",
    "        \"\"\"\n",
    "        Re-creates the generator on an updated model.\n",
    "        \"\"\"\n",
    "        sig = signature(self.recourse_method.__init__)\n",
    "        params = [param for param in sig.parameters]\n",
    "        \n",
    "        if 'data' in params:\n",
    "            self.generator = self.recourse_method(mlmodel=self.model,\n",
    "                                                  data=self.dataset,\n",
    "                                                  hyperparams=self.generator_params)\n",
    "        else:\n",
    "            self.generator = self.recourse_method(mlmodel=self.model,\n",
    "                                                  hyperparams=self.generator_params)\n",
    "       \n",
    "    \n",
    "    def apply(self, factuals):\n",
    "        \"\"\"\n",
    "        Generate (a set of) counterfactuals with a method relevant for the generator.\n",
    "        \n",
    "        Args:\n",
    "            factuals (pandas.DataFrame): \n",
    "                One or more records from a dataset used to train the black-box model.\n",
    "                \n",
    "        Returns:\n",
    "            int: Number of successfully generated counterfactuals.\n",
    "        \"\"\"\n",
    "        if self.timeout is not None:\n",
    "            return self.apply_recourse_with_timeout(factuals)\n",
    "        else:\n",
    "            return self.apply_recourse(factuals)\n",
    "            \n",
    "            \n",
    "    def apply_recourse(self, factuals):\n",
    "        \"\"\"\n",
    "        Generate (a set of) counterfactual explanations with a provided generator.\n",
    "        \n",
    "        Args:\n",
    "            factuals (pandas.DataFrame): \n",
    "                One or more records from a dataset used to train the black-box model.\n",
    "                \n",
    "        Returns:\n",
    "            int: Number of successfully generated counterfactuals.\n",
    "        \"\"\"\n",
    "        \n",
    "        log.info(f\"Applying the {self.name} generator.\")\n",
    "        \n",
    "        if factuals is None or len(factuals) == 0:\n",
    "            return None\n",
    "                                              \n",
    "        counterfactuals = self.generator.get_counterfactuals(factuals).dropna()\n",
    "        self.dataset._df.update(counterfactuals)\n",
    "        \n",
    "        return counterfactuals\n",
    "  \n",
    "                              \n",
    "    def apply_recourse_with_timeout(self, factuals):\n",
    "        \"\"\"\n",
    "        Generate (a set of) counterfactual explanations with a provided generator. \n",
    "        These explanations are applied one-by-one with a specific timeout for every single factual.\n",
    "        \n",
    "        Args:\n",
    "            factuals (pandas.DataFrame): \n",
    "                One or more records from a dataset used to train the black-box model.\n",
    "                \n",
    "        Returns:\n",
    "            int: Number of successfully generated counterfactuals.\n",
    "        \"\"\"\n",
    "        \n",
    "        log.info(f\"Applying the {self.name} generator.\")\n",
    "        \n",
    "        if factuals is None or len(factuals) == 0:\n",
    "            return None\n",
    "              \n",
    "        found_counterfactuals = None\n",
    "        for i in range(len(factuals)):\n",
    "            f = factuals.iloc[[i]]\n",
    "            log.info(f\"Generating counterfactual {i + 1} with {self.name}\")\n",
    "            # CARLA does not implement a timeout for the generators by default\n",
    "            # but we want to prevent the code from running indefinitely\n",
    "            counterfactual = recourse_controller(recourse_worker, self.timeout, self.generator, f)\n",
    "            # We only want to overwrite the existing data if counterfactual generation was successful\n",
    "            if counterfactual is not None and not counterfactual.empty:\n",
    "                self.dataset._df.iloc[f.index[0]] = counterfactual.iloc[0]\n",
    "                counterfactual.rename(index={0:f.index[0]}, inplace=True)\n",
    "                if found_counterfactuals is None:\n",
    "                    found_counterfactuals = counterfactual\n",
    "                else:\n",
    "                    found_counterfactuals = pd.concat([found_counterfactuals, counterfactual], axis=0)       \n",
    "        return found_counterfactuals\n",
    "    \n",
    "    \n",
    "    def update_model(self):\n",
    "        \"\"\"\n",
    "        Re-train the model based on an updated dataset.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Ensure that the dataset saved by the model is always updated with counterfactuals\n",
    "        self.model.data._df.update(self.dataset._df)\n",
    "        self.model.data._df_train.update(self.dataset._df)\n",
    "                              \n",
    "        log.info(f'Updating the {self.name} model')   \n",
    "        self.model = train_model(self.dataset, self.model_params, retrain=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "be0674bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: What to do if a generator times out? do we accept different numbers of samples?\n",
    "# TODO: Add benchmarks\n",
    "class RecourseExperiment():\n",
    "    \"\"\"\n",
    "    Allows to conduct an experiment about the dynamics of algorithmic recourse.\n",
    "    \n",
    "    Attributes:\n",
    "        dataset (DataCatalog): \n",
    "            Catalog containing a dataframe, set of train and test records, and the target.\n",
    "        model (MLModelCatalog) \n",
    "            Classifier with additional utilities required by CARLA.\n",
    "        generators (List[RecourseGenerator]):\n",
    "            List of one or more generators which will be measured in the experiment.\n",
    "        experiment_name (str):\n",
    "            Name of the experiment that will be used as part of the directory name where results are saved.\n",
    "    \"\"\"    \n",
    "    def __init__(self, dataset, model, generators, experiment_name='experiment'):\n",
    "        assert len(generators) != 0\n",
    "    \n",
    "        # Experiment data is saved into a new directory\n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
    "        self.experiment_name = f'{timestamp}_{experiment_name}'\n",
    "        os.makedirs(f'../experiment_data/{self.experiment_name}')\n",
    "        self.initial_dataset = deepcopy(dataset._df)\n",
    "        self.initial_model = deepcopy(model)\n",
    "        self.generators = generators\n",
    "\n",
    "        # factuals are a list of instances that the model expects to belong to the negative class;\n",
    "        # in order to acurately measure the performance of the dataset we never change the test set\n",
    "        self.factuals = predict_negative_instances(model, dataset.df_train)\n",
    "        self.factuals_index = self.factuals.index.tolist()\n",
    "        \n",
    "        pos_individuals = dataset.df_train.loc[dataset.df_train['target'] == dataset.positive]\n",
    "        self.initial_pos_sample = pos_individuals.sample(n=min(len(pos_individuals), 100)).to_numpy()\n",
    "        \n",
    "        self.experiment_data = {}\n",
    "        self.benchmarks = {}\n",
    "        for g in self.generators:\n",
    "            self.experiment_data[g.name] = {0: {}}\n",
    "            self.benchmarks[g.name] = DynamicBenchmark(model, g.recourse_method, g, self.factuals)\n",
    "            \n",
    "    def run(self, total_recourse=0.8, recourse_per_epoch=0.01):\n",
    "        \"\"\"\n",
    "        Driver code to execute an experiment that allows to compare the dynamics of recourse \n",
    "        applied by some generator to a benchmark described by Wachter et al. (2017).\n",
    "        \n",
    "        Attributes:\n",
    "            total_recourse (float): \n",
    "                Value between 0 and 1 representing the proportion of samples from the training set\n",
    "                which should have recourse applied to them throughout the experiment.\n",
    "            recourse_per_epoch (float): \n",
    "                Value between 0 and 1 representing the proportion of samples from the training set\n",
    "                which should have recourse applied to them in a single iteration.\n",
    "        \"\"\"\n",
    "        path = f'../experiment_data/{self.experiment_name}'\n",
    "        \n",
    "        # Convert ratio of samples that should undergo recourse in a single epoch into a number\n",
    "        recourse_per_epoch = max(int(recourse_per_epoch * len(self.factuals)), 1)\n",
    "        # Convert ratio of samples that should undergo recourse in total into a number of epochs\n",
    "        epochs = max(int(min(total_recourse, 1) * len(self.factuals) / recourse_per_epoch), 1)\n",
    "        \n",
    "        for g in self.generators:\n",
    "            self.benchmarks[g.name].start(self.experiment_data, path, self.initial_dataset,\n",
    "                                          self.initial_model, self.initial_pos_sample)\n",
    "                          \n",
    "        for epoch in range(epochs - 1):\n",
    "            log.info(f\"Starting epoch {epoch + 1}\")\n",
    "            # Generate a subset S of factuals that have never been encountered by the generators\n",
    "            sample_size = min(recourse_per_epoch, len(self.factuals_index))\n",
    "            current_factuals_index = np.random.choice(self.factuals_index, replace=False, size=sample_size)\n",
    "            # We do not want to accidentally generate a counterfactual from a counterfactual\n",
    "            self.factuals_index = [f for f in self.factuals_index if f not in current_factuals_index] \n",
    "            \n",
    "            # Apply the same set of actions on all generators passed to the experiment\n",
    "            for g in self.generators:\n",
    "                self.benchmarks[g.name].next_iteration(self.experiment_data, path, current_factuals_index,\n",
    "                                                       self.initial_dataset, self.initial_model, \n",
    "                                                       self.initial_pos_sample)\n",
    "                \n",
    "\n",
    "        # Measure the quality of recourse\n",
    "        self.experiment_data['evaluation'] = {}\n",
    "        for g in self.generators:\n",
    "            success_rate = g.num_found / max(len(self.factuals.index) - len(self.factuals_index), 1)\n",
    "            self.experiment_data['evaluation'][g.name] = {'success_rate': success_rate}\n",
    "            \n",
    "                         \n",
    "    def save_data(self, path=None):\n",
    "        \"\"\"\n",
    "        Write the data collected throughout the experiment into a .json file.\n",
    "        \n",
    "        Args:\n",
    "            path (str):\n",
    "                Directory where the dictionary of experiment data should be written.\n",
    "        \"\"\"\n",
    "        \n",
    "        path = path or f'../experiment_data/{self.experiment_name}/measurements.json'\n",
    "        with open(path, 'w') as outfile:\n",
    "            json.dump(self.experiment_data, outfile, sort_keys=True, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0e3e7ae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DynamicBenchmark(Benchmark):\n",
    "    def __init__(self, mlmodel, recourse_method, generator, factuals):\n",
    "        self._mlmodel = mlmodel\n",
    "        self._recourse_method = recourse_method\n",
    "        self._generator = generator\n",
    "        self._factuals = factuals.copy()\n",
    "        self._epoch = 0\n",
    "        self._timer = 0\n",
    "\n",
    "        # Avoid using scaling and normalizing more than once\n",
    "        if isinstance(mlmodel, MLModelCatalog):\n",
    "            self._mlmodel.use_pipeline = False  # type: ignore\n",
    "            \n",
    "            \n",
    "    def start(self, experiment_data, path, initial_dataset, initial_model, initial_pos_sample):\n",
    "        experiment_data[self._generator.name][0] = measure(self._generator,\n",
    "                                                           initial_model,\n",
    "                                                           initial_pos_sample,\n",
    "                                                           self._epoch)\n",
    "\n",
    "\n",
    "        # Plot initial data distributions\n",
    "        plot_distribution(self._generator.dataset, initial_dataset, self._generator.model, path,\n",
    "                          self._generator.name, 'distribution', self._epoch)\n",
    "        \n",
    "            \n",
    "    def next_iteration(self, experiment_data, path, current_factuals_index,\n",
    "                       initial_dataset, initial_model, initial_pos_sample):\n",
    "        start_time = timeit.default_timer()\n",
    "        \n",
    "        experiment_data[self._generator.name][self._epoch + 1] = {}\n",
    "                \n",
    "        # Find relevant factuals\n",
    "        current_factuals = self._generator.dataset._df.iloc[current_factuals_index]\n",
    "\n",
    "        # Apply recourse\n",
    "        counterfactuals = self._generator.apply(current_factuals)\n",
    "\n",
    "        self._generator.num_found += len(counterfactuals.index)\n",
    "        self._generator.update_model()\n",
    "\n",
    "        # Measure the data distribution and performance of the model\n",
    "        experiment_data[self._generator.name][self._epoch + 1] = measure(self._generator,\n",
    "                                                                         initial_model,\n",
    "                                                                         initial_pos_sample,\n",
    "                                                                         self._epoch + 1)\n",
    "\n",
    "        # Plot data distributions\n",
    "        plot_distribution(self._generator.dataset, initial_dataset, self._generator.model, path,\n",
    "                          self._generator.name, 'distribution', self._epoch + 1, counterfactuals)\n",
    "\n",
    "        # Re-create the generator on new model\n",
    "        self._generator.update_generator()\n",
    "        \n",
    "        self._epoch += 1\n",
    "        self._timer += timeit.default_timer() - start_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "92ea8ce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def measure(generator, initial_model, initial_pos_sample, epoch):\n",
    "    \"\"\"\n",
    "    Quantify the dataset and model and save into `experiment_data`.\n",
    "\n",
    "    Args:\n",
    "        generator (RecourseGenerator):\n",
    "            Recourse generator along with utilities required to conduct experiments.\n",
    "        epoch (int): \n",
    "            Current epoch in the experiment.\n",
    "    \"\"\"  \n",
    "    results = {}\n",
    "\n",
    "    # Measure the distributions of data\n",
    "    results['distribution'] = measure_distribution(generator.dataset)\n",
    "\n",
    "    # Measure the current performance of models\n",
    "    results['performance'] = performance(generator.dataset, generator.model)\n",
    "\n",
    "    # Measure the disagreement between current model and the initial model\n",
    "    results['disagreement'] = disagreement_distance(generator.dataset._df_test, generator.dataset.target,\n",
    "                                                    initial_model, generator.model)\n",
    "    \n",
    "    results['avg_distance'] = boundary_distance(generator.dataset, generator.model)\n",
    "\n",
    "    # Measure the MMD\n",
    "    results['MMD'] = current_MMD(generator.dataset._df, generator.dataset._positive, initial_pos_sample)  \n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "324acae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def block_print():\n",
    "    \"\"\"\n",
    "    Reroute stdout to disable printing.\n",
    "    \"\"\"\n",
    "    sys.stdout = open(os.devnull, 'w')\n",
    "\n",
    "def enable_print():\n",
    "    \"\"\"\n",
    "    Activate printing again.\n",
    "    \"\"\"\n",
    "    sys.stdout = sys.__stdout__"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CSE3000",
   "language": "python",
   "name": "cse3000"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
