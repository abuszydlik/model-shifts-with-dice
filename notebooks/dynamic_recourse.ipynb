{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f608fd63",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", message=r\"Passing\", category=FutureWarning)\n",
    "warnings.filterwarnings(\"ignore\", message=r\"Implicit\", category=UserWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "24d0e397",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Using Python-MIP package version 1.12.0 [model.py <module>]\n"
     ]
    }
   ],
   "source": [
    "import carla.models.catalog.load_model as loading_utils\n",
    "import carla.models.catalog.train_model as training_utils\n",
    "import json\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "from carla import log\n",
    "from carla.models.negative_instances import predict_negative_instances\n",
    "from carla.recourse_methods import Dice, Wachter\n",
    "from carla import MLModelCatalog\n",
    "from carla.data.catalog import DataCatalog\n",
    "from copy import deepcopy\n",
    "from datetime import datetime\n",
    "from func_timeout import func_timeout, FunctionTimedOut\n",
    "from ipynb.fs.full.metrics import current_MMD, disagreement_distance, measure_distribution, performance\n",
    "from ipynb.fs.full.plotting import plot_distribution\n",
    "from sklearn.model_selection import train_test_split\n",
    "from typing import List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1ca8213e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DynamicCsvCatalog(DataCatalog):\n",
    "    \"\"\"\n",
    "    Wrapper class for the DataCatalog similar to the built-in CsvCatalog but with new capabilities\n",
    "    required to control data in the experiments.\n",
    "    \n",
    "    Attributes:\n",
    "        file_path (str): \n",
    "            Path to the .csv file containing the dataset.\n",
    "        categorical (List[str]): \n",
    "            Names of columns describing categorical features.\n",
    "        continuous (List[str]):\n",
    "            Names of columns describing continuous (i.e. numerical) features.\n",
    "        immutables (List[str]):\n",
    "            Names of columns describing immutable features, not supported by all generators.\n",
    "        target (str):\n",
    "            Name of the column that contains the target variable.\n",
    "        test_size (float):\n",
    "            Proportion of the dataset which should be withheld as an independent test set.\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    def __init__(self, file_path: str, categorical: List[str],  continuous: List[str],\n",
    "                 immutables: List[str], target: str, test_size: float,\n",
    "                 scaling_method: str = \"MinMax\", encoding_method: str = \"OneHot_drop_binary\",\n",
    "                 positive=1, negative=0):\n",
    "        \n",
    "        self._categorical = categorical\n",
    "        self._continuous = continuous\n",
    "        self._immutables = immutables\n",
    "        self._target = target\n",
    "        self._positive = positive\n",
    "        self._negative = negative\n",
    "\n",
    "        # Load the raw data\n",
    "        raw = pd.read_csv(file_path)\n",
    "        train_raw, test_raw = train_test_split(raw, test_size=test_size)\n",
    "\n",
    "        super().__init__(\"custom\", raw, train_raw, test_raw,\n",
    "                         scaling_method, encoding_method)\n",
    "\n",
    "    @property\n",
    "    def categorical(self) -> List[str]:\n",
    "        return self._categorical\n",
    "\n",
    "    @property\n",
    "    def continuous(self) -> List[str]:\n",
    "        return self._continuous\n",
    "\n",
    "    @property\n",
    "    def immutables(self) -> List[str]:\n",
    "        return self._immutables\n",
    "\n",
    "    @property\n",
    "    def target(self) -> str:\n",
    "        return self._target\n",
    "    \n",
    "    @property\n",
    "    def positive(self) -> str:\n",
    "        return self._positive\n",
    "    \n",
    "    @property\n",
    "    def negative(self) -> str:\n",
    "        return self._negative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7c73de88",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DynamicMLModelCatalog(MLModelCatalog):\n",
    "    \"\"\"\n",
    "    Wrapper class for the MLModelCatalog that introduces additional functions\n",
    "    allowing for the efficient and unbiased measurement of the dynamics of recourse.\n",
    "    \n",
    "    Attributes:\n",
    "        data (DataCatalog):\n",
    "            Dataset which will be used to train a model and conduct experiments.\n",
    "        model_type (str):\n",
    "            Black-box model used for classification, currently this class supports only ANNs and Logistic Regression.\n",
    "        backend (str):\n",
    "            Specifies the framework used on the backend, currently this class supports only PyTorch.\n",
    "        cache (Boolean):\n",
    "            If True, the framework will attempt to load a model that was previously cached.\n",
    "        models_home (str):\n",
    "            Path to the directory where models should be saved after they are trained.\n",
    "        load_online: (Boolean):\n",
    "            If True, a pretrained model will be loaded.\n",
    "        kwargs (dict):\n",
    "            Dictionary of optional keyworded arguments.\n",
    "    \"\"\"\n",
    "    def __init__(self, data: DataCatalog, model_type: str, backend: str = \"pytorch\",\n",
    "        cache: bool = True, models_home: str = None, load_online: bool = True, **kwargs) -> None:\n",
    "        \n",
    "        if backend != 'pytorch':\n",
    "            raise NotImplementedError(f\"Only PyTorch models are currently supported\")\n",
    "            \n",
    "        if model_type not in ['ann', 'linear']:\n",
    "            raise NotImplementedError(f\"Model type not supported: {self.model_type}\")\n",
    "            \n",
    "        save_name = model_type\n",
    "        if model_type == \"ann\":\n",
    "            save_name += f\"_layers_{kwargs['save_name_params']}\"\n",
    "            \n",
    "        self._save_name = save_name\n",
    "        \n",
    "        super().__init__(data, model_type, backend, cache,\n",
    "                         models_home, load_online, **kwargs)\n",
    "    \n",
    "    @property\n",
    "    def save_name(self) -> str:\n",
    "        return self._save_name\n",
    "        \n",
    "    def params(self):        \n",
    "        # Attempt to load the saved model\n",
    "        self._model = loading_utils.load_trained_model(save_name=self._save_name,\n",
    "                                                       data_name=self.data.name,\n",
    "                                                       backend=self.backend)\n",
    "        \n",
    "        # This method should only be used when a model is already available\n",
    "        if self._model is None:\n",
    "            raise ValueError(f\"No trained model found for {self._save_name}\")\n",
    "            \n",
    "        for param in self._model.parameters():\n",
    "            print(param)\n",
    "        \n",
    "    def retrain(self, learning_rate=0.01, epochs=5, batch_size=1, hidden_size=[4]):\n",
    "        \"\"\"\n",
    "        Loads a cached model and retrains it on an updated dataset.\n",
    "        \n",
    "        Args:\n",
    "            learning_rate (float):\n",
    "                Size of the step at each epoch of the model training.\n",
    "            epochs (int):\n",
    "                Number of iterations of training.\n",
    "            batch_size (int):\n",
    "                Number of samples used at once in a gradient descent step, if '1' the procedure is stochastic.    \n",
    "        \"\"\"\n",
    "\n",
    "        # Attempt to load the saved model\n",
    "        self._model = loading_utils.load_trained_model(save_name=self._save_name,\n",
    "                                                       data_name=self.data.name,\n",
    "                                                       backend=self.backend)\n",
    "        \n",
    "        # This method should only be used when a model is already available\n",
    "        if self._model is None:\n",
    "            raise ValueError(f\"No trained model found for {self._save_name}\")\n",
    "        \n",
    "        # Sanity check to see if loaded model accuracy makes sense\n",
    "        if self._model is not None:\n",
    "            self._test_accuracy()\n",
    "            \n",
    "        # Get preprocessed data\n",
    "        df_train = self.data.df_train\n",
    "        df_test = self.data.df_test\n",
    "\n",
    "        # All dataframes may have possibly changed\n",
    "        x_train = df_train[list(set(df_train.columns) - {self.data.target})]\n",
    "        y_train = df_train[self.data.target]\n",
    "        x_test = df_test[list(set(df_test.columns) - {self.data.target})]\n",
    "        y_test = df_test[self.data.target]\n",
    "\n",
    "        # Order data (column-wise) before training\n",
    "        x_train = self.get_ordered_features(x_train)\n",
    "        x_test = self.get_ordered_features(x_test)\n",
    "        \n",
    "        log.info(f\"Current balance: train set {y_train.mean()}, test set {y_test.mean()}\")\n",
    "        \n",
    "        # Access the data in a format expected by PyTorch\n",
    "        train_dataset = training_utils.DataFrameDataset(x_train, y_train)\n",
    "        train_loader = training_utils.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "        test_dataset = training_utils.DataFrameDataset(x_test, y_test)\n",
    "        test_loader = training_utils.DataLoader(test_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "        # Retrain the model\n",
    "        training_utils._training_torch(self._model, train_loader, test_loader,\n",
    "                                       learning_rate, epochs)\n",
    "\n",
    "        loading_utils.save_model(model=self._model, save_name=self._save_name,\n",
    "                                 data_name=self.data.name, backend=self.backend)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab619bcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RecourseGenerator():\n",
    "    \n",
    "    def __init__(self, name, dataset, model, recourse_method, params):\n",
    "        self.name = name\n",
    "        self.dataset = dataset\n",
    "        self.model = model\n",
    "        self.recourse_method = recourse_method\n",
    "        self.params = params\n",
    "        \n",
    "        self.update_generator()\n",
    "        \n",
    "    def update_generator():\n",
    "        self.generator = recourse_method(self.model, params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "be0674bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Generalize for other recourse generators\n",
    "# TODO: What to do if a generator times out? do we accept different numbers of samples?\n",
    "# TODO: Create a wrapper for RecourseMethod to allow for the measurement of multiple generators at once\n",
    "class RecourseExperiment():\n",
    "    \"\"\"\n",
    "    Allows to conduct an experiment about the dynamics of algorithmic recourse.\n",
    "    \n",
    "    Attributes:\n",
    "        dataset (DataCatalog): \n",
    "            Catalog containing a dataframe, set of train and test records, and the target.\n",
    "        generator_name (str):\n",
    "            Name of the generator that is placed under test.\n",
    "        generator_timeout (int): \n",
    "            Number of seconds after which the search for counterfactuals will time out.\n",
    "        experiment_name (str):\n",
    "            Name of the experiment that will be used as part of the directory name where results are saved.\n",
    "        kwargs (dict):\n",
    "            Dictionary of optional keyworded arguments.  \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, dataset, total_recourse=0.5, recourse_per_epoch=0.01, generator_name='DICE',\n",
    "                 positive_class=1, negative_class=0, generator_timeout=60, experiment_name='experiment', **kwargs):\n",
    "    \n",
    "        # Experiment data is saved into a new directory\n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
    "        self.experiment_name = f'{timestamp}_{experiment_name}'\n",
    "        os.makedirs(f'../experiment_data/{self.experiment_name}')\n",
    "        self.generator_name = generator_name\n",
    "        self.generator_timeout = generator_timeout\n",
    "\n",
    "        self.experiment_data = {\n",
    "            self.generator_name: {0: {}},\n",
    "            'wachter': {0: {}}\n",
    "        }\n",
    "\n",
    "        # Train a classifier on the dataset\n",
    "        self.hyper_params = kwargs.get('hyper_params',\n",
    "                                       {'learning_rate': 0.04, 'epochs': 3, 'batch_size': 1, 'hidden_size': [4]})\n",
    "        self.dataset = dataset\n",
    "        self.model = train_model(dataset, self.hyper_params)\n",
    "        self.initial_model = deepcopy(self.model)\n",
    "\n",
    "        # Recourse generated by DICE is compared with the Wachter generator, as they may modify data differently\n",
    "        # we need to keep track of two models and two datasets and update them independently\n",
    "        self.benchmark_dataset = deepcopy(dataset)\n",
    "        self.benchmark_model = deepcopy(self.model)\n",
    "\n",
    "        # factuals are a list of instances that the model expects to belong to the negative class;\n",
    "        # in order to acurately measure the performance of the dataset we never change the test set\n",
    "        self.factuals = predict_negative_instances(self.model, dataset.df_train)\n",
    "        self.factuals_index = self.factuals.index.tolist()\n",
    "\n",
    "        # Instantiate recourse generators\n",
    "        self.dice_params = kwargs.get('generator_params', {\"num\": 1})\n",
    "        self.benchmark_params = kwargs.get('benchmark_params', {\"loss_type\": \"BCE\", \"t_max_min\": 0.5})\n",
    "        \n",
    "        self.generator = Dice(self.model, self.dice_params)\n",
    "        self.benchmark = Wachter(self.benchmark_model, self.benchmark_params)\n",
    "        \n",
    "        pos_individuals = dataset.df_train.loc[dataset.df_train['target'] == positive_class]\n",
    "        self.initial_pos_sample = pos_individuals.sample(n=min(len(pos_individuals), 100)).to_numpy()\n",
    "            \n",
    "    def run(self, total_recourse=0.8, recourse_per_epoch=0.01):\n",
    "        \"\"\"\n",
    "        Driver code to execute an experiment that allows to compare the dynamics of recourse \n",
    "        applied by some generator to a benchmark described by Wachter et al. (2017).\n",
    "        \n",
    "        Attributes:\n",
    "            total_recourse (float): \n",
    "                Value between 0 and 1 representing the proportion of samples from the training set\n",
    "                which should have recourse applied to them throughout the experiment.\n",
    "            recourse_per_epoch (float): \n",
    "                Value between 0 and 1 representing the proportion of samples from the training set\n",
    "                which should have recourse applied to them in a single iteration.\n",
    "        \"\"\"\n",
    "    \n",
    "        num_found_generator = 0\n",
    "        num_found_benchmark = 0\n",
    "           \n",
    "        # Measure the data distribution and performance of models\n",
    "        self.measure(0)\n",
    "        \n",
    "        # Plot initial data distributions\n",
    "        path = f'../experiment_data/{self.experiment_name}'\n",
    "        plot_distribution(self.dataset._df, self.model, path,\n",
    "                  self.generator_name, 'distribution', 0)\n",
    "        plot_distribution(self.benchmark_dataset._df, self.benchmark_model, path,\n",
    "                          'wachter', 'distribution', 0)\n",
    "        \n",
    "        # Convert ratio of samples that should undergo recourse in a single epoch into a number\n",
    "        recourse_per_epoch = max(int(recourse_per_epoch * len(self.factuals)), 1)\n",
    "        # Convert ratio of samples that should undergo recourse in total into a number of epochs\n",
    "        epochs = max(int(min(total_recourse, 1) * len(self.factuals) / recourse_per_epoch), 1)\n",
    "                              \n",
    "        for epoch in range(epochs - 1):\n",
    "            log.info(f\"Starting epoch {epoch + 1}\")\n",
    "            self.experiment_data[self.generator_name][epoch + 1] = {}\n",
    "            self.experiment_data['wachter'][epoch + 1] = {}\n",
    "                              \n",
    "            # 4. Generate a subset S of factuals that have never been encountered by the generators\n",
    "            sample_size = min(recourse_per_epoch, len(self.factuals_index))\n",
    "            current_factuals_index = np.random.choice(self.factuals_index, replace=False, size=sample_size)\n",
    "            current_test_factuals = self.dataset._df.iloc[current_factuals_index]\n",
    "            current_benchmark_factuals = self.benchmark_dataset._df.iloc[current_factuals_index] \n",
    "                              \n",
    "            # We do not want to accidentally generate a counterfactual from a counterfactual\n",
    "            self.factuals_index = [f for f in self.factuals_index if f not in current_factuals_index]  \n",
    "\n",
    "            # Apply recourse for S with DICE\n",
    "            num_found_generator += self.apply_recourse_with_timeout(self.dataset,\n",
    "                                                                    current_test_factuals,\n",
    "                                                                    self.generator)\n",
    "\n",
    "            # Update DICE model\n",
    "            self.model = self.update_model(self.dataset, self.model,\n",
    "                                           self.hyper_params, self.generator_name)\n",
    "\n",
    "\n",
    "            # Apply recourse for S with Wachter\n",
    "            num_found_benchmark += self.apply_recourse(self.benchmark_dataset,\n",
    "                                                       current_benchmark_factuals,\n",
    "                                                       self.benchmark)\n",
    "            \n",
    "            # Update the Wachter model\n",
    "            self.benchmark_model = self.update_model(self.benchmark_dataset, self.benchmark_model,\n",
    "                                                     self.hyper_params, 'Wachter')\n",
    "                           \n",
    "            # Measure the data distribution and performance of models\n",
    "            self.measure(epoch + 1)\n",
    "\n",
    "            # Plot data distributions\n",
    "            plot_distribution(self.dataset._df, self.model, path,\n",
    "                              self.generator_name, 'distribution', epoch + 1)\n",
    "            plot_distribution(self.benchmark_dataset._df, self.benchmark_model, path,\n",
    "                              'wachter', 'distribution', epoch + 1)\n",
    "            \n",
    "            self.generator = Dice(self.model, self.dice_params)\n",
    "            self.benchmark = Wachter(self.benchmark_model, self.benchmark_params)\n",
    "\n",
    "        # Measure the quality of recourse\n",
    "        self.experiment_data['evaluation'] = {\n",
    "            'dice': {\n",
    "                'success_rate': num_found_generator / max(len(self.factuals.index) - len(self.factuals_index), 1)\n",
    "            },\n",
    "            'wachter': {\n",
    "                'success_rate': num_found_benchmark / max(len(self.factuals.index) - len(self.factuals_index), 1)\n",
    "            }\n",
    "        }\n",
    "        \n",
    "    def measure(self, epoch):\n",
    "        \"\"\"\n",
    "        Quantify the dataset and model and save into `experiment_data`.\n",
    "        \n",
    "        Args:\n",
    "            epoch (int): \n",
    "                Current epoch in the experiment.\n",
    "        \"\"\"     \n",
    "        # Measure the distributions of data\n",
    "        self.experiment_data[self.generator_name][epoch]['distribution'] = measure_distribution(self.dataset)\n",
    "        self.experiment_data['wachter'][epoch]['distribution'] = measure_distribution(self.benchmark_dataset)\n",
    "\n",
    "        # Measure the current performance of models\n",
    "        self.experiment_data[self.generator_name][epoch][\"performance\"] = performance(self.dataset,\n",
    "                                                                         self.model)\n",
    "        self.experiment_data['wachter'][epoch][\"performance\"] = performance(self.benchmark_dataset,\n",
    "                                                                            self.benchmark_model)   \n",
    "        \n",
    "        # Measure the disagreement between current models and the initial model\n",
    "        generator_disagreement = disagreement_distance(self.dataset._df_test, self.dataset.target,\n",
    "                                                       self.initial_model, self.model)\n",
    "        self.experiment_data[self.generator_name][epoch]['disagreement'] = generator_disagreement\n",
    "        \n",
    "        benchmark_disagreement = disagreement_distance(self.dataset._df_test, self.dataset.target,\n",
    "                                                       self.initial_model, self.benchmark_model)\n",
    "        self.experiment_data['wachter'][epoch]['disagreement'] = benchmark_disagreement\n",
    "        \n",
    "        print(f'benchmark {benchmark_disagreement} -- DICE {generator_disagreement}')\n",
    "        \n",
    "        # Measure the MMD\n",
    "        self.experiment_data[self.generator_name][epoch]['MMD'] = current_MMD(self.dataset._df_train,\n",
    "                                                                              self.dataset._positive,\n",
    "                                                                              self.initial_pos_sample)\n",
    "        \n",
    "        self.experiment_data['wachter'][epoch]['MMD'] = current_MMD(self.benchmark_dataset._df_train,\n",
    "                                                                    self.benchmark_dataset._positive,\n",
    "                                                                    self.initial_pos_sample)\n",
    "                         \n",
    "            \n",
    "    def update_model(self, dataset, model, hyper_params, generator_name):\n",
    "        \"\"\"\n",
    "        Re-train the model based on an updated dataset\n",
    "        \n",
    "        Args:\n",
    "            dataset (DataCatalog): \n",
    "                Catalog containing a dataframe, set of train and test records, and the target.\n",
    "            model (MLModelCatalog) \n",
    "                Classifier with additional utilities required by CARLA.\n",
    "            generator_name (str):\n",
    "                Name of the generator that has its model updated.\n",
    "                \n",
    "        Returns:\n",
    "            MLModelCatalog: A re-trained classifier.\n",
    "        \"\"\"\n",
    "        # Ensure that the dataset saved by the model is always updated with counterfactuals\n",
    "        model.data._df.update(dataset._df)\n",
    "        model.data._df_train.update(dataset._df)\n",
    "                              \n",
    "        log.info(f'Updating the {generator_name} model')                      \n",
    "        return train_model(dataset, hyper_params, retrain=True)                    \n",
    "        \n",
    "                                    \n",
    "    def apply_recourse(self, dataset, factuals, generator, generator_name='Wachter'):\n",
    "        \"\"\"\n",
    "        Generate (a set of) counterfactual explanations with a provided generator.\n",
    "        \n",
    "        Args:\n",
    "            dataset (DataCatalog): \n",
    "                Catalog containing a dataframe, set of train and test records, and the target.\n",
    "            factuals (pandas.DataFrame): \n",
    "                One or more records from a dataset used to train the black-box model.\n",
    "            generator (RecourseMethod): \n",
    "                Generator that finds counterfactual explanations using a black-box model.\n",
    "            generator_name (str):\n",
    "                Name of the generator that is used to apply recourse.\n",
    "                \n",
    "        Returns:\n",
    "            int: Number of successfully generated counterfactuals.\n",
    "        \"\"\"\n",
    "        log.info(f\"Applying the {generator_name} generator.\")\n",
    "        \n",
    "        if factuals is None or len(factuals) == 0:\n",
    "            return 0\n",
    "                                              \n",
    "        counterfactuals = generator.get_counterfactuals(factuals).dropna()\n",
    "        dataset._df.update(counterfactuals)\n",
    "        \n",
    "        return len(counterfactuals.index)\n",
    "  \n",
    "                              \n",
    "    def apply_recourse_with_timeout(self, dataset, factuals, generator, generator_name='DICE'):\n",
    "        \"\"\"\n",
    "        Generate (a set of) counterfactual explanations with a provided generator. \n",
    "        These explanations are applied one-by-one with a specific timeout for every single factual.\n",
    "        \n",
    "        Args:\n",
    "            dataset (DataCatalog): \n",
    "                Catalog containing a dataframe, set of train and test records, and the target.\n",
    "            factuals (pandas.DataFrame): \n",
    "                One or more records from a dataset used to train the black-box model.\n",
    "            generator (RecourseMethod): \n",
    "                Generator that finds counterfactual explanations using a black-box model.\n",
    "            generator_name (str):\n",
    "                Name of the generator that is used to apply recourse.\n",
    "                \n",
    "        Returns:\n",
    "            int: Number of successfully generated counterfactuals.\n",
    "        \"\"\"\n",
    "        log.info(f\"Applying the {generator_name} generator.\")\n",
    "        \n",
    "        if factuals is None or len(factuals) == 0:\n",
    "            return 0\n",
    "                              \n",
    "        counterfactuals_found = 0\n",
    "        for i in range(len(factuals)):\n",
    "            f = factuals.iloc[[i]]\n",
    "            \n",
    "            log.info(f\"Generating counterfactual {i + 1} with {generator_name}\")\n",
    "            # CARLA does not implement a timeout for the generators by default\n",
    "            # but we want to prevent the code from running indefinitely\n",
    "            counterfactuals = recourse_controller(recourse_worker, self.generator_timeout, generator, f)\n",
    "            # We only want to overwrite the existing data if counterfactual generation was successful\n",
    "            if counterfactuals is not None and not counterfactuals.empty:\n",
    "                dataset._df.iloc[f.index[0]] = counterfactuals.iloc[0]\n",
    "                counterfactuals_found += 1\n",
    "                         \n",
    "        return counterfactuals_found\n",
    "                \n",
    "                         \n",
    "    def save_data(self, path=None):\n",
    "        \"\"\"\n",
    "        Write the data collected throughout the experiment into a .json file.\n",
    "        \n",
    "        Args:\n",
    "            path (str):\n",
    "                Directory where the dictionary of experiment data should be written.\n",
    "        \"\"\"\n",
    "        \n",
    "        path = path or f'../experiment_data/{self.experiment_name}/measurements.json'\n",
    "        with open(path, 'w') as outfile:\n",
    "            json.dump(self.experiment_data, outfile, sort_keys=True, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a1ca82e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def recourse_worker(generator, factual):\n",
    "    \"\"\"\n",
    "    Apply algorithmic recourse for a (set of) factuals using a chosen generator.\n",
    "    \n",
    "    Args:\n",
    "        generator (RecourseMethod): \n",
    "            Generator that finds counterfactual explanations using a black-box model.\n",
    "        factual (pandas.DataFrame): \n",
    "            One or more records from a dataset used to train the black-box model.\n",
    "        \n",
    "    Returns:\n",
    "        pandas.DataFrame: A counterfactual explanation for the provided factual.\n",
    "    \"\"\"\n",
    "    if factual is None:\n",
    "        raise ValueException('Provided with a non-existent factual')\n",
    "        return None\n",
    "    \n",
    "    counterfactuals = generator.get_counterfactuals(factual).dropna()\n",
    "    if not counterfactuals.empty:\n",
    "        return counterfactuals.sample().astype(float)\n",
    "    raise FunctionTimedOut()\n",
    "    \n",
    "def recourse_controller(function, max_wait_time, generator, factual):\n",
    "    \"\"\"\n",
    "    Wrapper function that ensures the application of recourse does not run indefinitely.\n",
    "    \n",
    "    Args:\n",
    "        function (Callable): \n",
    "            Function that will have its execution placed under a timeout.\n",
    "        max_wait_time (int): \n",
    "            Number of seconds after which the `function` will time out.\n",
    "        generator (RecourseMethod): \n",
    "            Generator that finds counterfactual explanations using a black-box model.\n",
    "        factual (pandas.DataFrame): \n",
    "            One or more records from a dataset used to train the black-box model.\n",
    "        \n",
    "    Returns:\n",
    "        pandas.DataFrame: A counterfactual explanation for the provided factual if found.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        return func_timeout(max_wait_time, function, args=[generator, factual]) \n",
    "    except FunctionTimedOut:\n",
    "        log.info(\"Timeout - No Counterfactual Explanation Found\")\n",
    "\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1d30e100",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(dataset, hyper_params, model_type='ann', retrain=False):\n",
    "    \"\"\"\n",
    "    Instantiates and trains a black-box model within a CARLA wrapper that will be used to generate explanations.\n",
    "    \n",
    "    Args:\n",
    "        dataset (DataCatalog): \n",
    "            Catalog containing a dataframe, set of train and test records, and the target.\n",
    "        hyper_params (dict): \n",
    "            Dictionary storing all custom hyper-parameter values for the model.\n",
    "        \n",
    "    Returns:\n",
    "        MLModelCatalog: \n",
    "            Classifier with additional utilities required by CARLA.\n",
    "    \"\"\"\n",
    "    kwargs = {'save_name_params': \"_\".join([str(size) for size in hyper_params['hidden_size']])}\n",
    "    model = DynamicMLModelCatalog(dataset, model_type=model_type,\n",
    "                                  load_online=False, backend=\"pytorch\", **kwargs)\n",
    "\n",
    "    # force_train is enabled to ensure that the model is not reused from the cache\n",
    "    if not retrain:\n",
    "        log.info(\"Training the initial model\")\n",
    "        model.train(learning_rate=hyper_params['learning_rate'],\n",
    "                    epochs=hyper_params['epochs'],\n",
    "                    batch_size=hyper_params['batch_size'],\n",
    "                    hidden_size=hyper_params['hidden_size'],\n",
    "                    force_train=True)\n",
    "    else:\n",
    "        model.retrain(learning_rate=hyper_params['learning_rate'],\n",
    "                      epochs=hyper_params['epochs'],\n",
    "                      batch_size=hyper_params['batch_size'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67a4fe67",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CSE3000",
   "language": "python",
   "name": "cse3000"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
